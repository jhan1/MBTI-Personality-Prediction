Overall: 86/100

1. ( 5/5) Abstract: Does the abstract summarize the central question, methods,
and conclusions of your work? Is the abstract concise?

  - slightly more detail about results might be nice, but overall basically does what it needs to and is a reasonable length


2. ( 13/15) Introduction: Does the paper introduce the key issues/central
questions (the what), motivate the work (the why), and clearly outline the
approach taken to address these questions (the how)?

  - would be good to spend a bit more time on the "how" part; in particular, to
    put your work into the context of other related work.  This can involve
    things like "why the classifiers we picked are appropriate for this data",
    or "other people looking at this data have found X, so we hope to look for
    Y", etc.

  - basic explanation of MBTI and motivation is at an appropriate level of
    detail


3. ( 24/25) Methodology: Is your algorithmic approach clearly described? Are the
experiments clearly described (key parameter settings, data set, evaluation
metrics) so that they can be replicated? Are figures/equations/pseudcode used
where appropriate? Is the approach suitable/justified to address the main
question? Is the paper appropriate for the target audience?

  - generally contains the things it should; level of detail for methods is fine
    for this context

  - unclear why single-run holdout validation was used; why not use k-fold
    instead?  if there's a reason for this it should be explained

  - also it would be good to explain why each of these models was chosen; i.e.
    why is this a sensible thing to apply to your data, why did you think it
    would produce interesting results you could learn something from, etc.

  - for hyperparameter tuning section, we don't know what the "baseline" numbers
    are, so when you say it "increased to X" the reader doesn't know how
    substantial that increase is, or whether it's statistically meaningful, etc.

14. ( 18/25) Results and Discussion: Are the results clearly presented and
analyzed? Are graphs and tables used where appropriate?

  - results table and graphs are all fine in terms of formatting

  - given that you did a single run of hold-out validation, I'm not sure you can
    really claim SVM produces better accuracy than KNN or Logistic Regression

  - precision/recall/F1 are all binary classification metrics, but you're doing
    multi-class here; how did you define and calculate those things in this
    context?  without an explanation of what they mean here, it's hard to
    interpret those numbers

  - also it would be nice to see some confusion matrices (maybe not for every
    technique, but a couple of interesting ones that you want to talk about)

  - remember, overfitting means the training error goes down while the testing
    error goes up; just doing better on train than on test doesn't mean it's
    overfitting (almost every model will almost always do better on train than
    test).  None of these plots shows signs of overfitting, just that there's a
    point of diminishing returns in each case.

  - there really isn't much in the way of discussion or analysis here; you
    describe the results in text form, but you do very little to help the reader
    understand or contextualize those results, draw inferences about the nature
    of your data or models, etc.  Basically, you've done a fine job of the
    Results section, but effectively don't have a Discussion section.


5. ( 14/15) Social Implications: Have key stakeholders been identified? Are the
implications of this work on those stakeholders identified and analyzed? On the
relationships between those stakeholders? Does the analysis consider multiple
perspectives?

  - mostly reasonable, though some of these things are probably more serious
    than they're presented as; e.g. you say "*if* our dataset is not
    representative", as though you hadn't previously pointed out that your data
    comes from volunteerism on a forum for people who are interested in
    personality types; this is *very* unlikely to be a representative sample of
    the population as a whole.

  - I also think you may be underestimating the potential to use something like
    this for targeting of misinformation, spearphishing, etc.; e.g. by knowing
    personality type, I may be able to send each user misinformation in a way
    that user is most likely to be vulnerable to, etc.  You actually mention
    something like this in the Conclusions section, but fail to note how easy it
    would be to abuse such a system.


6. ( 9/10) Is there a substantive conclusion? Are future directions for this
work described?

  - conclusion is fine, future work is a bit weaker (there are lots of much
    simpler and more straightforward things you could do before jumping to deep
    learning)


7. ( 3/5) Is there a bibliography provided with complete citations to relevant
papers?

  - many of your references are incomplete; they don't give any indication of
    publisher/venue/URL/etc..  Just author/title/date is not a complete
    citation.
      - for some things this is just a result of not the correct reference types
        in the .bib file, e.g. using "@Book" for a website

      - in many cases though the information we need is simply missing

  - also, scholarly sources like textbooks, academic papers, etc. tend to be
    preferred over web-based resources in this type of writing.  This is both
    because it's hard to be sure if content on the web has been properly vetted,
    and because web content may not be stable (that is, I can probably find a
    copy of an older book, but an old webpage may have vanished forever).
